# Recent papers on multi-fractal stochastic Ito calculus as a core methodology or framework

- **On Construction, Properties and Simulation of Haar-Based Multifractional Processes**
   Antoine Ayache, Andriy Olenko, Nemini Samarakoon
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:276928946)
   <details>
     <summary> Abstract </summary>
     Multifractional processes extend the concept of fractional Brownian motion by replacing the constant Hurst parameter with a time-varying Hurst function. This extension allows for modulation of the roughness of sample paths over time. The paper introduces a new class of multifractional processes, the Gaussian Haar-based multifractional processes (GHBMP), which is based on the Haar wavelet series representations. The resulting processes cover a significantly broader set of Hurst functions compared to the existing literature, enhancing their suitability for both practical applications and theoretical studies. The theoretical properties of these processes are investigated. Simulation studies conducted for various Hurst functions validate the proposed model and demonstrate its applicability, even for Hurst functions exhibiting discontinuous behaviour.
  </details>

- **Dimensionality reduction can be used as a surrogate model for high-dimensional forward uncertainty quantification**
   Jungho Kim, Sangri Yi, Ziqi Wang
   Reliability Engineering & System Safety 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:267523127)
   <details>
     <summary> Abstract </summary>
     We introduce a method to construct a stochastic surrogate model from the results of dimensionality reduction in forward uncertainty quantification. The hypothesis is that the high-dimensional input augmented by the output of a computational model admits a low-dimensional representation. This assumption can be met by numerous uncertainty quantification applications with physics-based computational models. The proposed approach differs from a sequential application of dimensionality reduction followed by surrogate modeling, as we"extract"a surrogate model from the results of dimensionality reduction in the input-output space. This feature becomes desirable when the input space is genuinely high-dimensional. The proposed method also diverges from the Probabilistic Learning on Manifold, as a reconstruction mapping from the feature space to the input-output space is circumvented. The final product of the proposed method is a stochastic simulator that propagates a deterministic input into a stochastic output, preserving the convenience of a sequential"dimensionality reduction + Gaussian process regression"approach while overcoming some of its limitations. The proposed method is demonstrated through two uncertainty quantification problems characterized by high-dimensional input uncertainties.
  </details>

- **TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting**
   Vladyslav Moroshan, Julien N. Siems, Arber Zela, Timur Carstensen, Frank Hutter
   arXiv.org 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:282574554)
   <details>
     <summary> Abstract </summary>
     Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.
  </details>

- **The scaling of stochastic physical systems with applications in electrodynamics**
   Keith Davey, Raul Ochoa-Cabrero, Zainab Safaa Ali, Jiahe Xu
   Journal of Engineering Mathematics 2025
   [open paper page](https://doi.org/10.1007/s10665-025-10458-3)
   <details>
     <summary> Abstract </summary>
     
  </details>

- **A Unified Theory of $\theta$-Expectations**
   Qian Qi
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:280324074)
   <details>
     <summary> Abstract </summary>
     We derive a new class of non-linear expectations from first-principles deterministic chaotic dynamics. The homogenization of the system's skew-adjoint microscopic generator is achieved using the spectral theory of transfer operators for uniformly hyperbolic flows. We prove convergence in the viscosity sense to a macroscopic evolution governed by a fully non-linear Hamilton-Jacobi-Bellman (HJB) equation. Our central result establishes that the HJB Hamiltonian possesses a rigid structure: affine in the Hessian but demonstrably non-convex in the gradient. This defines a new $\theta$-expectation and constructively establishes a class of non-convex stochastic control problems fundamentally outside the sub-additive framework of G-expectations.
  </details>

- **Stochastic Calculus via Stopping Derivatives**
   A. Simpson
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:275336470)
   <details>
     <summary> Abstract </summary>
     We show that a substantial portion of stochastic calculus can be developed along similar lines to ordinary calculus, with derivative-based concepts driving the development. We define a notion of stopping derivative, which is a form of right derivative with respect to stopping times. Using this, we define the drift and variance rate of a process as stopping derivatives for (generalised) conditional expectation and conditional variance respectively. Applying elementary, derivative-based methods, we derive a calculus of rules describing how drift and variance rate transform under constructions on processes, culminating in a version of the multi-dimensional It\^o formula. Our approach connects with the standard machinery of stochastic calculus via a theorem establishing that continuous processes with zero drift coincide with random translations of continuous local martingales. This equivalence allows us to derive a Fundamental Theorem of Calculus for stopping derivatives, which relates the quantities of drift and variance rate, defined as stopping derivatives, to parameters used in the description of a process as a stochastic integral.
  </details>

- **Linear Quadratic Control with Non-Markovian and Non-Semimartingale Noise Models**
   Mostafa M. Shibl, Sharan Srinivasan, Harsha Honnappa, Vijay Gupta
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:283693281)
   <details>
     <summary> Abstract </summary>
     The standard linear quadratic Gaussian (LQG) framework assumes a Brownian noise process and relies on classical stochastic calculus tools, such as those based on It\^o calculus. In this paper, we solve a generalized linear quadratic optimal control problem where the process and measurement noises can be non-Markovian and non-semimartingale stochastic processes with sample paths that have low H\"older regularity. Since these noise models do not, in general, permit the use of the standard It\^o calculus, we employ rough path theory to formulate and solve the problem. By leveraging signature representations and controlled rough paths, we derive the optimal state estimation and control strategies.
  </details>

- **Masked Completion via Structured Diffusion with White-Box Transformers**
   Druv Pai, Ziyang Wu, Sam Buchanan, Yaodong Yu, Yi Ma
   International Conference on Learning Representations 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:268875803)
   <details>
     <summary> Abstract </summary>
     Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving a deep transformer-like masked autoencoder architecture, called CRATE-MAE, in which the role of each layer is mathematically fully interpretable: they transform the data distribution to and from a structured representation. Extensive empirical evaluations confirm our analytical insights. CRATE-MAE demonstrates highly promising performance on large-scale imagery datasets while using only ~30% of the parameters compared to the standard masked autoencoder with the same model configuration. The representations learned by CRATE-MAE have explicit structure and also contain semantic meaning. Code is available at https://github.com/Ma-Lab-Berkeley/CRATE .
  </details>

- **The memory-dependent FPK equation for fractional Gaussian noise**
   Lifang Feng, B. Pei, Yong Xu
   Probabilistic Engineering Mechanics 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:282719045)
   <details>
     <summary> Abstract </summary>
     unknown
  </details>

- **Unbiased Rough Integrators and No Free Lunch in Rough-Path-Based Market Models**
   Tomoyuki Ichiba, Qijin Shi
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:281394346)
   <details>
     <summary> Abstract </summary>
     Built to generalise classical stochastic calculus, rough path theory provides a natural and pathwise framework to model continuous non-semimartingale assets. This paper investigates the ultimate capacity of this framework to support frictionless continuous No-Free-Lunch markets \`a la Kreps-Yan. We establish a ``Rough Kreps-Yan"theorem, which links our No Controlled Free Lunch (NCFL) condition to the unbiasedness of the driver of the price process as a rough integrator. The central work of this paper is a complete classification of these unbiased rough integrators with respect to different classes of controlled paths as integrands. As the set of admissible trading strategies is enlarged to include Markovian-type and signature-type portfolios, the only admissible random rough paths must be infinitesimally close to the It\^o rough path lift of a standard Brownian motion, up to a time change. In particular, Gaussianity is no longer a model assumption, but rather a no-arbitrage market consequence. Notably, simple strategies do not appear in the theory, and if they are then reintroduced, the rough noise is further enforced to be the It\^o rough path of Brownian motion itself. Ultimately, this implies that continuous frictionless markets based on rough path theory are inevitably constrained to the semimartingale paradigm, providing a definitive answer on the limits of this approach. Our framework covers $\alpha-$H\"older continuous rough paths for $\alpha>0$ arbitrarily small.
  </details>

- **Diffusion Models as Cartoonists: The Curious Case of High Density Regions**
   Rafal Karczewski, Markus Heinonen, Vikas Garg
   International Conference on Learning Representations 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:273811850)
   <details>
     <summary> Abstract </summary>
     We investigate what kind of images lie in the high-density regions of diffusion models. We introduce a theoretical mode-tracking process capable of pinpointing the exact mode of the denoising distribution, and we propose a practical high-density sampler that consistently generates images of higher likelihood than usual samplers. Our empirical findings reveal the existence of significantly higher likelihood samples that typical samplers do not produce, often manifesting as cartoon-like drawings or blurry images depending on the noise level. Curiously, these patterns emerge in datasets devoid of such examples. We also present a novel approach to track sample likelihoods in diffusion SDEs, which remarkably incurs no additional computational cost. Code is available at https://github.com/Aalto-QuML/high-density-diffusion.
  </details>

- **Ito's formula for flows of conditional measures on semimartingales**
   Xin Guo, Jiacheng Zhang
   unknown 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:269187945)
   <details>
     <summary> Abstract </summary>
     Motivated by recent development of mean-field systems with common noise, this paper establishes Ito's formula for flows of conditional probability measures under a common filtration associated with general semimartingales. This generalizes existing works on flows of conditional measures on Ito processes and flows of deterministic measure on general semimartingales. The key technical components involve constructing conditional independent copies and establishing the equivalence between stochastic integrals with respect to the conditional law of semimartingales and the conditional expectation of stochastic integrals with respect to copies of semimartingales. Ito's formula is then established for cylindrical functions through conditional independent copies, and extended to the general case through function approximations.
  </details>

- **Geometric Rough Paths above Mixed Fractional Brownian Motion**
   Atef Lechiheb
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:283244981)
   <details>
     <summary> Abstract </summary>
     This paper establishes a comprehensive theory of geometric rough paths for mixed fractional Brownian motion (MFBM) and its generalized multi-component extensions. We prove that for a generalized MFBM of the form $M_t^H(a) = \sum_{k=1}^N a_k B_t^{H_k}$ with $\min\{H_k\}>\frac{1}{4}$, there exists a canonical geometric rough path obtained as the limit of smooth rough paths associated with dyadic approximations. This extends the classical result of Coutin and Qian \cite{coutin2002} for single fractional Brownian motion to the mixed case. We provide explicit bounds on the $p$-variation norms and establish a Skorohod integral representation connecting our pathwise construction to the Malliavin calculus framework. Furthermore, we demonstrate applications to rough differential equations driven by MFBM, enabling the use of Lyons'universal limit theorem for this class of processes. Finally, we study the signature of MFBM paths, providing a complete algebraic characterization of their geometric properties. Our approach unifies the treatment of multiple fractional components and reveals the fundamental interactions between different regularity scales, completing the rough path foundation for mixed fractional processes with applications in stochastic analysis and beyond.
  </details>

- **Tanaka formula for SDEs driven by fractional Brownian motion**
   T. Sottinen, Ercan Sonmez, L. Viitasaari
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:280561763)
   <details>
     <summary> Abstract </summary>
     We derive a Tanaka-type formula for the solution of a stochastic differential equation (SDE) driven by fractional Brownian motion (fBm) with Hurst parameter $H>\frac{1}{2}$. While Tanaka formulas for the fractional Brownian motion itself have been established, a corresponding result for non-linear SDEs driven by fBm has so far been unavailable. Our formula reveals a structure not previously observed: it features both a Skorokhod integral and a Malliavin trace correction, where the analogue of the local time appears through a double integral involving the Dirac distribution and the Malliavin derivative of the solution. A second double integral captures the variation of the diffusion coefficient along the flow. A key step in our analysis is a novel method to establish $L^2$-convergence of the trace term, which avoids the use of white noise calculus and instead exploits Gaussian-type density estimates for the law of the solution. The result applies to a broad class of equations under suitable regularity assumptions and extends naturally to convex functionals. As special cases, we recover known identities for the fractional Brownian motion and the fractional Ornstein--Uhlenbeck process.
  </details>

- **Parameter Estimation of Long Memory Stochastic Processes with Deep Neural Networks**
   Bálint Csanády, Lóránt Nagy, Dániel Boros, Iván Ivkovic, D'avid Kov'acs, Dalma Tóth-Lakits, Zsolt László Márkus, András Lukács
   European Conference on Artificial Intelligence 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:273186016)
   <details>
     <summary> Abstract </summary>
     We present a purely deep neural network-based approach for estimating long memory parameters of time series models that incorporate the phenomenon of long-range dependence. Parameters, such as the Hurst exponent, are critical in characterizing the long-range dependence, roughness, and self-similarity of stochastic processes. The accurate and fast estimation of these parameters holds significant importance across various scientific disciplines, including finance, physics, and engineering. We harnessed efficient process generators to provide high-quality synthetic training data, enabling the training of scale-invariant 1D Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) models. Our neural models outperform conventional statistical methods, even those augmented with neural networks. The precision, speed, consistency, and robustness of our estimators are demonstrated through experiments involving fractional Brownian motion (fBm), the Autoregressive Fractionally Integrated Moving Average (ARFIMA) process, and the fractional Ornstein-Uhlenbeck (fOU) process. We believe that our work will inspire further research in the field of stochastic process modeling and parameter estimation using deep learning techniques.
  </details>

- **Non-markovian dynamics: the memory-dependent probability density evolution equations**
   B. Pei, Lifang Feng, Yunzhang Li, Yong Xu
   Nonlinear dynamics 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:273962807)
   <details>
     <summary> Abstract </summary>
     This paper aims to investigate the non-Markovian dynamics. The governing equations are derived for the probability density functions (PDFs) of non-Markovian stochastic responses to Langevin equation excited by combined fractional Gaussian noise (FGN) and Gaussian white noise (GWN). The main difficulty here is that the Langevin equation excited by FGN cannot be augmented by a filter excited by GWN, leading to the inapplicability of Itô stochastic calculus theory. Thus, in the present work, based on the fractional Wick Itô Skorohod integral and rough path theory, a new non-Markovian probability density evolution method is established to derive theoretically the memory-dependent probability density evolution equation (PDEEs) for the PDFs of non-Markovian stochastic responses to Langevin equation excited by combined FGN and GWN, which is a breakthrough to stochastic dynamics. Then, we extend an efficient algorithm, the local discontinuous Galerkin method, to numerically solve the memory-dependent PDEEs. Remarkably, this proposed method attains a higher accuracy compared to the prevalent methods such as finite difference, path integral (PI) and Monte Carlo methods, and boasts a broader applicability than the PI method, which fails to solve the memory-dependent PDEEs. Finally, several numerical examples are illustrated to verify the proposed scheme.
  </details>

- **Functional Stochastic Gradient MCMC for Bayesian Neural Networks**
   Mengjing Wu, Junyu Xuan, Jie Lu
   International Conference on Artificial Intelligence and Statistics 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:272880945)
   <details>
     <summary> Abstract </summary>
     Classical parameter-space Bayesian inference for Bayesian neural networks (BNNs) suffers from several unresolved prior issues, such as knowledge encoding intractability and pathological behaviours in deep networks, which can lead to improper posterior inference. To address these issues, functional Bayesian inference has recently been proposed leveraging functional priors, such as the emerging functional variational inference. In addition to variational methods, stochastic gradient Markov Chain Monte Carlo (MCMC) is another scalable and effective inference method for BNNs to asymptotically generate samples from the true posterior by simulating continuous dynamics. However, existing MCMC methods perform solely in parameter space and inherit the unresolved prior issues, while extending these dynamics to function space is a non-trivial undertaking. In this paper, we introduce novel functional MCMC schemes, including stochastic gradient versions, based on newly designed diffusion dynamics that can incorporate more informative functional priors. Moreover, we prove that the stationary measure of these functional dynamics is the target posterior over functions. Our functional MCMC schemes demonstrate improved performance in both predictive accuracy and uncertainty quantification on several tasks compared to naive parameter-space MCMC and functional variational inference.
  </details>

- **It\^o formula for reduced rough paths**
   Nannan Li, Xing Gao
   unknown 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:281421330)
   <details>
     <summary> Abstract </summary>
     The It\^o formula, also known as the change-of-variables formula, is a cornerstone of It\^o stochastic calculus. Over time, this formula has been extended to apply to random processes for which classical calculus is insufficient. Since every random process exhibits some degree of regularity, rough path theory provides a natural framework for treating them uniformly. In this paper, we extend the It\^o formula for reduced rough paths, broadening the range of roughness from the previously known case $\frac{1}{3}<\alpha \leq \frac{1}{2}$ to the more singular regime $\frac{1}{4}<\alpha \leq \frac{1}{3}$.
  </details>

- **Perception-based multiplicative noise removal using SDEs**
   Anh Xuan Vuong, Thinh Nguyen
    2024
   [open paper page](https://api.semanticscholar.org/CorpusId:271909730)
   <details>
     <summary> Abstract </summary>
     Multiplicative noise, also known as speckle or pepper noise, commonly affects images produced by synthetic aperture radar (SAR), lasers, or optical lenses. Unlike additive noise, which typically arises from thermal processes or external factors, multiplicative noise is inherent to the system, originating from the fluctuation in diffuse reflections. These fluctuations result in multiple copies of the same signal with varying magnitudes being combined. Consequently, despeckling, or removing multiplicative noise, necessitates different techniques compared to those used for additive noise removal. In this paper, we propose a novel approach using Stochastic Differential Equations based diffusion models to address multiplicative noise. We demonstrate that multiplicative noise can be effectively modeled as a Geometric Brownian Motion process in the logarithmic domain. Utilizing the Fokker-Planck equation, we derive the corresponding reverse process for image denoising. To validate our method, we conduct extensive experiments on two different datasets, comparing our approach to both classical signal processing techniques and contemporary CNN-based noise removal models. Our results indicate that the proposed method significantly outperforms existing methods on perception-based metrics such as FID and LPIPS, while maintaining competitive performance on traditional metrics like PSNR and SSIM.
  </details>

- **Bridging Geometric States via Geometric Diffusion Bridge**
   Shengjie Luo, Yixian Xu, Di He, Shuxin Zheng, Tie-Yan Liu, Liwei Wang
   Neural Information Processing Systems 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:273707694)
   <details>
     <summary> Abstract </summary>
     The accurate prediction of geometric state evolution in complex systems is critical for advancing scientific domains such as quantum chemistry and material modeling. Traditional experimental and computational methods face challenges in terms of environmental constraints and computational demands, while current deep learning approaches still fall short in terms of precision and generality. In this work, we introduce the Geometric Diffusion Bridge (GDB), a novel generative modeling framework that accurately bridges initial and target geometric states. GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob's $h$-transform for connecting geometric states. This tailored diffusion process is anchored by initial and target geometric states as fixed endpoints and governed by equivariant transition kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB framework by using a chain of equivariant diffusion bridges, providing a more detailed and accurate characterization of evolution dynamics. Theoretically, we conduct a thorough examination to confirm our framework's ability to preserve joint distributions of geometric states and capability to completely model the underlying dynamics inducing trajectory distributions with negligible error. Experimental evaluations across various real-world scenarios show that GDB surpasses existing state-of-the-art approaches, opening up a new pathway for accurately bridging geometric states and tackling crucial scientific challenges with improved accuracy and applicability.
  </details>

- **An It\^o-type formula for some measure-valued processes and its application on controlled superprocesses**
   Shang Li
   unknown 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:273403883)
   <details>
     <summary> Abstract </summary>
     We derive an It\^o-type formula for a measure-valued process that has a decomposition analogous to a classical semimartingale. The derivation begins with a time partitioning approach similar to the classical proof of It\^o's formula. To address the new challenges arising from the measure-valued setting, we employ symmetric polynomials to approximate the second-order linear derivative of the functional on finite measures, alongside certain localization techniques. A controlled superprocess with a binary branching mechanism can be interpreted as a weak solution to a controlled stochastic partial differential equation (SPDE), which naturally leads to such a decomposition. Consequently, this It\^o-type formula makes it possible to derive the Hamilton-Jacobi-Bellman (HJB) equation and the verification theorem for controlled superprocesses with a binary branching mechanism. Additionally, we propose a heuristic definition for the viscosity solution of an equation involving derivatives on finite measures. We prove that a continuous value function is a viscosity solution in this sense and demonstrate the uniqueness of the viscosity solution when the second-order derivative term on the measure vanishes.
  </details>

- **A forward scheme with machine learning for forward-backward SDEs with jumps by decoupling jumps**
   Reiichiro Kawai, Riu Naito, Toshihiro Yamada
   arXiv.org 2024
   [open paper page](https://api.semanticscholar.org/CorpusId:273346651)
   <details>
     <summary> Abstract </summary>
     Forward-backward stochastic differential equations (FBSDEs) have been generalized by introducing jumps for better capturing random phenomena, while the resulting FBSDEs are far more intricate than the standard one from every perspective. In this work, we establish a forward scheme for potentially high-dimensional FBSDEs with jumps, taking a similar approach to [Bender and Denk, 117 (2007), Stoch. Process. Their Appl., pp.1793-1812], with the aid of machine learning techniques for implementation. The developed forward scheme is built upon a recursive representation that decouples random jumps at every step and converges exponentially fast to the original FBSDE with jumps, often requiring only a few iterations to achieve sufficient accuracy, along with the error bound vanishing for lower jump intensities. The established framework also holds novelty in its neural network-based implementation of a wide class of forward schemes for FBSDEs, notably whether with or without jumps. We provide an extensive collection of numerical results, showcasing the effectiveness of the proposed recursion and its corresponding forward scheme in approximating high-dimensional FBSDEs with jumps (up to 100-dimension) without directly handling the random jumps.
  </details>

- **How Particle-System Random Batch Methods Enhance Graph Transformer: Memory Efficiency and Parallel Computing Strategy**
   Hanwen Liu, Yixuan Ma, Shi Jin, Yuguang Wang
   arXiv.org 2025
   [open paper page](https://api.semanticscholar.org/CorpusId:282911811)
   <details>
     <summary> Abstract </summary>
     Attention mechanism is a significant part of Transformer models. It helps extract features from embedded vectors by adding global information and its expressivity has been proved to be powerful. Nevertheless, the quadratic complexity restricts its practicability. Although several researches have provided attention mechanism in sparse form, they are lack of theoretical analysis about the expressivity of their mechanism while reducing complexity. In this paper, we put forward Random Batch Attention (RBA), a linear self-attention mechanism, which has theoretical support of the ability to maintain its expressivity. Random Batch Attention has several significant strengths as follows: (1) Random Batch Attention has linear time complexity. Other than this, it can be implemented in parallel on a new dimension, which contributes to much memory saving. (2) Random Batch Attention mechanism can improve most of the existing models by replacing their attention mechanisms, even many previously improved attention mechanisms. (3) Random Batch Attention mechanism has theoretical explanation in convergence, as it comes from Random Batch Methods on computation mathematics. Experiments on large graphs have proved advantages mentioned above. Also, the theoretical modeling of self-attention mechanism is a new tool for future research on attention-mechanism analysis.
  </details>

- **Advancements in Fractional Neural Operators with Adaptive Hybrid Kernels in Multiscale Sobolev Spaces**
   Rômulo Damasclin Chaves dos Santos, Jorge Henrique de Oliveira Sales
   unknown 2025
   [open paper page](https://arxiv.org/pdf/2503.11680.pdf)
   <details>
     <summary> Abstract </summary>
     This paper introduces significant advancements in fractional neural operators (FNOs) through the integration of adaptive hybrid kernels and stochastic multiscale analysis. We address several open problems in the existing literature by establishing four foundational theorems. First, we achieve exact bias-variance separation for Riemann-Liouville operators using fractional Prokhorov metrics, providing a robust framework for handling non-local dependencies in differential equations. Second, we demonstrate L\'evy-regularized Hadamard stability with sharp convergence rates in Besov-Morrey spaces, enhancing FNO stability under heavy-tailed noise processes. Third, we overcome the curse of dimensionality in multivariate settings by achieving tensorized RL-Caputo convergence in RdRd with anisotropic H\"older regularity. Finally, we develop a quantum-inspired fractional gradient descent algorithm that significantly improves convergence rates in practical applications. Our proofs employ advanced techniques such as multiphase homogenization, Malliavin-Skorokhod calculus, and nonlocal divergence theorems, ensuring mathematical rigor and robustness. The practical implications of our work are demonstrated through applications in fusion plasma turbulence, where our methods yield a 70\% improvement over state-of-the-art FNOs. This enhancement is particularly notable in modeling the multifractal {\delta}B{\delta}B fields in ITER tokamak data, where the anisotropic penalty corresponds to enstrophy cascade rates in Hasegawa-Wakatani models, showcasing its versatility and potential for broader application.
  </details>
